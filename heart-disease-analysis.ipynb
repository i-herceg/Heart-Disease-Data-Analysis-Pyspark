{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0b6029d-69ff-4b00-b20e-baaeef9bb616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d9f2d4a-657c-48dc-8a18-a3211c03660c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/18 22:21:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Creating a Spark session\n",
    "# This initializes a Spark session which is the entry point to using Spark functionality.\n",
    "# The app name is set to \"HearDiseaseAnalysis\" for identifying the job in Spark UI.\n",
    "\n",
    "spark = SparkSession.builder.appName(\"HearDiseaseAnalysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0f55346-9207-4c1d-a438-093f4a897d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path for the dataset and output path for saving the result\n",
    "file_path = \"data/heart.csv\"  # Path to the heart disease dataset in CSV format\n",
    "output_path = \"output_parquet\"  # Path where output will be saved in Parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cce285e-d52d-4aac-9f41-5b6487027dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "|age|sex| cp|trestbps|chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|target|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "| 52|  1|  0|     125| 212|  0|      1|    168|    0|    1.0|    2|  2|   3|     0|\n",
      "| 53|  1|  0|     140| 203|  1|      0|    155|    1|    3.1|    0|  0|   3|     0|\n",
      "| 70|  1|  0|     145| 174|  0|      1|    125|    1|    2.6|    0|  0|   3|     0|\n",
      "| 61|  1|  0|     148| 203|  0|      1|    161|    0|    0.0|    2|  1|   3|     0|\n",
      "| 62|  0|  0|     138| 294|  1|      1|    106|    0|    1.9|    1|  3|   2|     0|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load the heart disease dataset\n",
    "    # Reads the CSV file into a DataFrame, with the header set to True to treat the first row as column names\n",
    "    # and inferSchema set to True to automatically detect the data types of each column.\n",
    "    df = spark.read.csv(file_path, header = True, inferSchema = True)\n",
    "\n",
    "    # Showing the first 5 rows of the dataset\n",
    "    # This gives us a preview of the loaded data to understand its structure and content.\n",
    "    df.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    # Error handling to capture any issues while loading the dataset\n",
    "    # If an error occurs, it will print a message with the exception details.\n",
    "    print(f\"Error loading data: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d05779de-874d-429a-beba-3461d29c4d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values by column\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "|age|sex| cp|trestbps|chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|target|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "|  0|  0|  0|       0|   0|  0|      0|      0|    0|      0|    0|  0|   0|     0|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+\n",
      "\n",
      "There are no missing values.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, col, when, count\n",
    "\n",
    "# Function for check and remove missing values\n",
    "def check_and_remove_missing_values(df):\n",
    "    try:\n",
    "        missing_counts = df.select(\n",
    "            [sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]\n",
    "        )\n",
    "\n",
    "        print(\"Missing values by column\")\n",
    "        missing_counts.show()\n",
    "\n",
    "        # If there are any missing values, remove them\n",
    "        if missing_counts.count() > 1:\n",
    "            df_cleaned = df.na.drop()\n",
    "            print(\"Column with missing values are removed\")\n",
    "            return df_cleaned  # Return the cleaned DataFrame\n",
    "        else:\n",
    "            print(\"There are no missing values.\")\n",
    "            return df # Return the original DataFrame if there are no missing values\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while checking for missing values: {e}\")\n",
    "    \n",
    "df = check_and_remove_missing_values(df) # Call the function on the DataFrame 'df'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c03ca3fc-1efd-45ac-86b7-4bb8f7f5e575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sex: integer (nullable = true)\n",
      " |-- cp: integer (nullable = true)\n",
      " |-- trestbps: integer (nullable = true)\n",
      " |-- chol: integer (nullable = true)\n",
      " |-- fbs: integer (nullable = true)\n",
      " |-- restecg: integer (nullable = true)\n",
      " |-- thalach: integer (nullable = true)\n",
      " |-- exang: integer (nullable = true)\n",
      " |-- oldpeak: double (nullable = true)\n",
      " |-- slope: integer (nullable = true)\n",
      " |-- ca: integer (nullable = true)\n",
      " |-- thal: integer (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema() #  Function in PySpark is used to print the schema of a DataFrame. It displays the column names, data types, and whether or not the column allows null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "030bd919-d9a4-48f7-b1ca-16a4943d249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check the values in a specific column and remove rows with invalid values\n",
    "def check_column_values(df, column_name, min_values, max_values):\n",
    "    try:\n",
    "        print(f\"Checking the values in column {column_name}:\")  # Inform the user which column is being checked\n",
    "\n",
    "        # Filtering rows where values are not within the specified range (min_values to max_values)\n",
    "        invalid_values = df.filter(~col(column_name).between(min_values, max_values))\n",
    "\n",
    "        # If there are invalid values, remove them from the DataFrame and display them\n",
    "        if invalid_values.count() > 0:\n",
    "            df = df.subtract(invalid_values)  # Remove rows with invalid values\n",
    "            print(f\"Inconsistent values in the column '{column_name}'\")\n",
    "            invalid_values.show()  # Show the rows with invalid values\n",
    "        else:\n",
    "            # If no invalid values are found, inform the user\n",
    "            print(f\"There are no inconsistent values in the column '{column_name}'\")\n",
    "\n",
    "        # Return the cleaned DataFrame (with invalid rows removed)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        # Catch and print any errors that occur during the validation process\n",
    "        print(f\"An error occurred while validating the column: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e11be531-3d83-4007-b3f1-18677342a563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the values in column age:\n",
      "There are no inconsistent values in the column 'age'\n",
      "Checking the values in column sex:\n",
      "There are no inconsistent values in the column 'sex'\n",
      "Checking the values in column cp:\n",
      "There are no inconsistent values in the column 'cp'\n",
      "Checking the values in column trestbps:\n",
      "There are no inconsistent values in the column 'trestbps'\n",
      "Checking the values in column chol:\n",
      "There are no inconsistent values in the column 'chol'\n",
      "Checking the values in column fbs:\n",
      "There are no inconsistent values in the column 'fbs'\n",
      "Checking the values in column restecg:\n",
      "There are no inconsistent values in the column 'restecg'\n",
      "Checking the values in column thalach:\n",
      "There are no inconsistent values in the column 'thalach'\n",
      "Checking the values in column exang:\n",
      "There are no inconsistent values in the column 'exang'\n",
      "Checking the values in column oldpeak:\n",
      "There are no inconsistent values in the column 'oldpeak'\n",
      "Checking the values in column slope:\n",
      "There are no inconsistent values in the column 'slope'\n",
      "Checking the values in column ca:\n",
      "There are no inconsistent values in the column 'ca'\n",
      "Checking the values in column thal:\n",
      "There are no inconsistent values in the column 'thal'\n",
      "Checking the values in column target:\n",
      "There are no inconsistent values in the column 'target'\n"
     ]
    }
   ],
   "source": [
    "# Checking and cleaning the 'age' column (valid range: 0-120)\n",
    "df = check_column_values(df, 'age', 0, 120)\n",
    "\n",
    "# Checking and cleaning the 'sex' column (valid values: 0 or 1)\n",
    "df = check_column_values(df, 'sex', 0, 1)\n",
    "\n",
    "# Checking and cleaning the 'cp' column (valid values: 0, 1, 2, 3)\n",
    "df = check_column_values(df, 'cp', 0, 3)\n",
    "\n",
    "# Checking and cleaning the 'trestbps' column (valid range: 0-300)\n",
    "df = check_column_values(df, 'trestbps', 0, 300)\n",
    "\n",
    "# Checking and cleaning the 'chol' column (valid range: 50-1000)\n",
    "df = check_column_values(df, 'chol', 50, 1000)\n",
    "\n",
    "# Checking and cleaning the 'fbs' column (valid values: 0 or 1)\n",
    "df = check_column_values(df, 'fbs', 0, 1)\n",
    "\n",
    "# Checking and cleaning the 'restecg' column (valid values: 0, 1, 2)\n",
    "df = check_column_values(df, 'restecg', 0, 2)\n",
    "\n",
    "# Checking and cleaning the 'thalach' column (valid range: 20-230)\n",
    "df = check_column_values(df, 'thalach', 20, 230)\n",
    "\n",
    "# Checking and cleaning the 'exang' column (valid values: 0 or 1)\n",
    "df = check_column_values(df, 'exang', 0, 1)\n",
    "\n",
    "# Checking and cleaning the 'oldpeak' column (valid range: -3 to 8)\n",
    "df = check_column_values(df, 'oldpeak', -3, 8)\n",
    "\n",
    "# Checking and cleaning the 'slope' column (valid values: 0, 1, 2)\n",
    "df = check_column_values(df, 'slope', 0, 2)\n",
    "\n",
    "# Checking and cleaning the 'ca' column (valid range: 0-4)\n",
    "df = check_column_values(df, 'ca', 0, 4)\n",
    "\n",
    "# Checking and cleaning the 'thal' column (valid range: 0-3)\n",
    "df = check_column_values(df, 'thal', 0, 3)\n",
    "\n",
    "# Checking and cleaning the 'target' column (valid values: 0 or 1)\n",
    "df = check_column_values(df, 'target', 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f0ede7c-c9e3-4b0b-8a03-876bdeb593e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "\n",
    "# When loading data, we set the parameter inferSchema = true and thus enabled \n",
    "# automatic determination of data types, but we can also do it manually\n",
    "\n",
    "# Implementation of data type conversion\n",
    "def data_type_conversion(df):\n",
    "    try:\n",
    "        # Converting columns to their appropriate data types (IntegerType or DoubleType)\n",
    "        df = df.withColumn(\"age\", col(\"age\").cast(IntegerType())) \\\n",
    "            .withColumn(\"sex\", col(\"sex\").cast(IntegerType())) \\\n",
    "            .withColumn(\"cp\", col(\"cp\").cast(IntegerType())) \\\n",
    "            .withColumn(\"trestbps\", col(\"trestbps\").cast(IntegerType())) \\\n",
    "            .withColumn(\"chol\", col(\"chol\").cast(IntegerType())) \\\n",
    "            .withColumn(\"fbs\", col(\"fbs\").cast(IntegerType())) \\\n",
    "            .withColumn(\"restecg\", col(\"restecg\").cast(IntegerType())) \\\n",
    "            .withColumn(\"thalach\", col(\"thalach\").cast(IntegerType())) \\\n",
    "            .withColumn(\"exang\", col(\"exang\").cast(IntegerType())) \\\n",
    "            .withColumn(\"oldpeak\", col(\"oldpeak\").cast(DoubleType())) \\\n",
    "            .withColumn(\"slope\", col(\"slope\").cast(IntegerType())) \\\n",
    "            .withColumn(\"ca\", col(\"ca\").cast(IntegerType())) \\\n",
    "            .withColumn(\"thal\", col(\"thal\").cast(IntegerType())) \\\n",
    "            .withColumn(\"target\", col(\"target\").cast(IntegerType()))\n",
    "        return df # Returning the DataFrame with updated column data types\n",
    "    except Exception as e:\n",
    "        # Printing error message if there's an issue during the conversion\n",
    "        print(f\"An error occurred while converting types: {e}\")\n",
    "\n",
    "# Applying the data type conversion function to the DataFrame\n",
    "df = data_type_conversion(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2ab1d41-b789-4d96-b8e7-b80691b38191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+------------------+---------+\n",
      "|age|sex| cp|trestbps|chol|fbs|restecg|thalach|exang|oldpeak|slope| ca|thal|target|high_risk_category|age_group|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+------------------+---------+\n",
      "| 52|  1|  0|     125| 212|  0|      1|    168|    0|    1.0|    2|  2|   3|     0|                 0|    50-60|\n",
      "| 53|  1|  0|     140| 203|  1|      0|    155|    1|    3.1|    0|  0|   3|     0|                 0|    50-60|\n",
      "| 70|  1|  0|     145| 174|  0|      1|    125|    1|    2.6|    0|  0|   3|     0|                 1|      60+|\n",
      "| 61|  1|  0|     148| 203|  0|      1|    161|    0|    0.0|    2|  1|   3|     0|                 1|      60+|\n",
      "| 62|  0|  0|     138| 294|  1|      1|    106|    0|    1.9|    1|  3|   2|     0|                 1|      60+|\n",
      "+---+---+---+--------+----+---+-------+-------+-----+-------+-----+---+----+------+------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved in Parquet format at output_parquet\n"
     ]
    }
   ],
   "source": [
    "# Add a new column 'high_risk_category' to categorize patients into high risk (1) or not (0)\n",
    "df = df.withColumn(\n",
    "    \"high_risk_category\",  # New column name\n",
    "    when(col(\"age\") > 60, 1)  # High risk if age is greater than 60\n",
    "    .when(col(\"chol\") > 250, 1)  # High risk if cholesterol is greater than 250\n",
    "    .when(col(\"trestbps\") > 140, 1)  # High risk if resting blood pressure is greater than 140\n",
    "    .when(col(\"thalach\") < 100, 1)  # High risk if maximum heart rate is less than 100\n",
    "    .otherwise(0)  # Otherwise, assign 0 (low risk)\n",
    ")\n",
    "\n",
    "# Create a new column 'age_group' based on the age of the patient\n",
    "df = df.withColumn(\n",
    "    \"age_group\",  # New column name\n",
    "    when(col(\"age\") < 30, \"Under 30\")  # Age group 'Under 30'\n",
    "    .when((col(\"age\") >= 30) & (col(\"age\") < 40), \"30-40\")  # Age group '30-40'\n",
    "    .when((col(\"age\") >= 40) & (col(\"age\") < 50), \"40-50\")  # Age group '40-50'\n",
    "    .when((col(\"age\") >= 50) & (col(\"age\") < 60), \"50-60\")  # Age group '50-60'\n",
    "    .when(col(\"age\") >= 60, \"60+\")  # Age group '60+'\n",
    "    .otherwise(\"Unknown\")  # Default value if age doesn't fit any of the categories\n",
    ")\n",
    "\n",
    "# Show the updated DataFrame with the new columns\n",
    "df.show(5)\n",
    "\n",
    "# Save the resulting DataFrame in Parquet format\n",
    "try:\n",
    "    # Writing the data to the specified output path in Parquet format\n",
    "    df.write.parquet(output_path)  \n",
    "    print(f\"Data successfully saved in Parquet format at {output_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    # Error handling if saving to Parquet fails\n",
    "    print(f\"An error occurred while saving to Parquet format: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5009523d-75a0-449f-9c72-ca011cdba2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation between high risk category and heart disease (target):\n",
      "+------------------+------+-----+\n",
      "|high_risk_category|target|count|\n",
      "+------------------+------+-----+\n",
      "|                 0|     0|  151|\n",
      "|                 0|     1|  251|\n",
      "|                 1|     0|  348|\n",
      "|                 1|     1|  275|\n",
      "+------------------+------+-----+\n",
      "\n",
      "High risk vs target distribution successfully saved in Parquet format.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Funkcija za analizu odnosa između visoko-rizičnih osoba i targeta\n",
    "def analyze_high_risk_vs_target(df):\n",
    "    try:\n",
    "        # Grupiranje podataka prema visokom riziku i targetu\n",
    "        result = df.groupBy(\"high_risk_category\", \"target\") \\\n",
    "            .agg(F.count(\"*\").alias(\"count\")) \\\n",
    "            .orderBy(\"high_risk_category\", \"target\")\n",
    "\n",
    "        print(\"Relation between high risk category and heart disease (target):\")\n",
    "        result.show()\n",
    "\n",
    "        # Spremanje rezultata u Parquet format\n",
    "        result.write.parquet(\"high_risk_vs_target_distribution.parquet\")\n",
    "        print(\"High risk vs target distribution successfully saved in Parquet format.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Poziv funkcije za analizu\n",
    "analyze_high_risk_vs_target(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0282995d-2b41-4598-b285-72f8f5ae066d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heart disease distribution by age and sex:\n",
      "+---------+---+------+-----+\n",
      "|age_group|sex|target|count|\n",
      "+---------+---+------+-----+\n",
      "| Under 30|  1|     1|    4|\n",
      "|    30-40|  0|     1|   17|\n",
      "|    30-40|  1|     0|   15|\n",
      "|    30-40|  1|     1|   21|\n",
      "|    40-50|  0|     0|    4|\n",
      "|    40-50|  0|     1|   55|\n",
      "|    40-50|  1|     0|   76|\n",
      "|    40-50|  1|     1|  102|\n",
      "|    50-60|  0|     0|   35|\n",
      "|    50-60|  0|     1|   74|\n",
      "|    50-60|  1|     0|  181|\n",
      "|    50-60|  1|     1|  132|\n",
      "|      60+|  0|     0|   47|\n",
      "|      60+|  0|     1|   80|\n",
      "|      60+|  1|     0|  141|\n",
      "|      60+|  1|     1|   41|\n",
      "+---------+---+------+-----+\n",
      "\n",
      "Heart disease distribution successfully saved in Parquet format.\n"
     ]
    }
   ],
   "source": [
    "# Function to analyze the distribution of heart disease by age group and sex\n",
    "def analyze_heart_disease_distribution(df):\n",
    "    try:\n",
    "        # Grouping data by age group, sex, and target (heart disease status), then calculating the count\n",
    "        result = df.groupBy(\"age_group\", \"sex\", \"target\") \\\n",
    "            .agg(F.count(\"*\").alias(\"count\"))  # Counting the occurrences in each group\n",
    "        \n",
    "        # Ordering the result by age group, sex, and target for better readability\n",
    "        result = result.orderBy(\n",
    "            F.when(F.col(\"age_group\") == \"Under 30\", 0)  # Assign numeric values to age groups for sorting\n",
    "             .when(F.col(\"age_group\") == \"30-40\", 1)\n",
    "             .when(F.col(\"age_group\") == \"40-50\", 2)\n",
    "             .when(F.col(\"age_group\") == \"50-60\", 3)\n",
    "             .when(F.col(\"age_group\") == \"60+\", 4)\n",
    "             .otherwise(5),\n",
    "            \"sex\",  # Sort by sex\n",
    "            \"target\"  # Sort by heart disease status (target)\n",
    "        )\n",
    "\n",
    "        # Displaying the results\n",
    "        print(\"Heart disease distribution by age and sex:\")\n",
    "        result.show()\n",
    "\n",
    "        # Saving the results in Parquet format for future use\n",
    "        result.write.parquet(\"heart_disease_distribution_by_age_and_sex.parquet\")\n",
    "        print(\"Heart disease distribution successfully saved in Parquet format.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Error handling in case of any issues during the analysis\n",
    "        print(f\"An error occurred during the analysis of heart disease distribution: {e}\")\n",
    "\n",
    "# Calling the function to analyze the distribution\n",
    "analyze_heart_disease_distribution(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58afd69c-c843-4eb3-bd0d-6c2ed53d6962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sex: integer (nullable = true)\n",
      " |-- cp: integer (nullable = true)\n",
      " |-- trestbps: integer (nullable = true)\n",
      " |-- chol: integer (nullable = true)\n",
      " |-- fbs: integer (nullable = true)\n",
      " |-- restecg: integer (nullable = true)\n",
      " |-- thalach: integer (nullable = true)\n",
      " |-- exang: integer (nullable = true)\n",
      " |-- oldpeak: double (nullable = true)\n",
      " |-- slope: integer (nullable = true)\n",
      " |-- ca: integer (nullable = true)\n",
      " |-- thal: integer (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      " |-- high_risk_category: integer (nullable = false)\n",
      " |-- age_group: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7df041f4-9e56-47c4-966d-9be5402d0719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/18 22:25:47 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All variable correlations successfully saved in Parquet format.\n",
      "Top 3 most significant factors correlated with heart disease:\n",
      "1. oldpeak - Correlation: 0.4384\n",
      "2. exang - Correlation: 0.4380\n",
      "3. cp - Correlation: 0.4349\n",
      "Top 3 most significant factors successfully saved in Parquet format.\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate and rank correlations of all numerical features with the target variable\n",
    "def calculate_all_correlations(df):\n",
    "    try:\n",
    "        # Extracting all numerical columns from the DataFrame, excluding 'target'\n",
    "        numerical_columns = [field.name for field in df.schema.fields if isinstance(field.dataType, (IntegerType, DoubleType)) and field.name != \"target\"]\n",
    "\n",
    "        # Initializing a dictionary to store correlations\n",
    "        correlations = {}\n",
    "\n",
    "        # Computing the correlation for each numerical variable with the target\n",
    "        for column in numerical_columns:\n",
    "            correlation_value = df.stat.corr(column, \"target\")\n",
    "            correlations[column] = abs(correlation_value)  # Absolute correlation value\n",
    "\n",
    "        # Sorting correlations in descending order\n",
    "        sorted_correlations = sorted(correlations.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "        # Saving all correlations to Parquet format\n",
    "        result_df = spark.createDataFrame(sorted_correlations, [\"Factor\", \"Correlation\"])\n",
    "        result_df.write.parquet(\"all_correlations.parquet\")\n",
    "        print(\"All variable correlations successfully saved in Parquet format.\")\n",
    "\n",
    "        # Displaying the top 3 most significant factors\n",
    "        print(\"Top 3 most significant factors correlated with heart disease:\")\n",
    "        for i in range(3):\n",
    "            factor, correlation = sorted_correlations[i]\n",
    "            print(f\"{i+1}. {factor} - Correlation: {correlation:.4f}\")\n",
    "\n",
    "        # Saving the top 3 most significant factors to Parquet format\n",
    "        top_3_df = spark.createDataFrame(sorted_correlations[:3], [\"Factor\", \"Correlation\"])\n",
    "        top_3_df.write.parquet(\"top_3_significant_factors.parquet\")\n",
    "        print(\"Top 3 most significant factors successfully saved in Parquet format.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handling errors during correlation analysis\n",
    "        print(f\"An error occurred during correlation analysis: {e}\")\n",
    "\n",
    "# Calling the function to analyze all correlations\n",
    "calculate_all_correlations(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f893005f-b2dc-4fcb-ac47-832ed259b1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risk profile counts by category:\n",
      "+---------+-------------+-----+\n",
      "|age_group| risk_profile|count|\n",
      "+---------+-------------+-----+\n",
      "|    50-60|     Low Risk|  245|\n",
      "|    50-60|    High Risk|   38|\n",
      "|    40-50|     Low Risk|  144|\n",
      "|    40-50|Moderate Risk|   78|\n",
      "| Under 30|     Low Risk|    4|\n",
      "|      60+|     Low Risk|  277|\n",
      "|    50-60|Moderate Risk|  139|\n",
      "|    40-50|    High Risk|   15|\n",
      "|      60+|Moderate Risk|   32|\n",
      "|    30-40|     Low Risk|   53|\n",
      "+---------+-------------+-----+\n",
      "\n",
      "Risk profile counts successfully saved in Parquet format.\n"
     ]
    }
   ],
   "source": [
    "# Function to create and count risk profiles based on age and other features\n",
    "def create_and_count_risk_profiles(df):\n",
    "    try:\n",
    "        # Create new risk profiles based on age and other health features\n",
    "        df = df.withColumn(\n",
    "            \"risk_profile\", \n",
    "            F.when((F.col(\"age_group\") == \"Under 30\") & (F.col(\"trestbps\") > 130) & (F.col(\"chol\") > 240), \"High Risk\")  # High risk condition for age < 30\n",
    "            .when((F.col(\"age_group\") == \"30-40\") & (F.col(\"trestbps\") > 140) & (F.col(\"thalach\") < 120), \"High Risk\")  # High risk condition for age between 30-40\n",
    "            .when((F.col(\"age_group\") == \"40-50\") & (F.col(\"chol\") > 250) & (F.col(\"thalach\") < 130), \"High Risk\")  # High risk condition for age between 40-50\n",
    "            .when((F.col(\"age_group\") == \"50-60\") & (F.col(\"trestbps\") > 150) & (F.col(\"chol\") > 260), \"High Risk\")  # High risk condition for age between 50-60\n",
    "            .when((F.col(\"age_group\") == \"60+\") & (F.col(\"trestbps\") > 160) & (F.col(\"thalach\") < 100), \"High Risk\")  # High risk condition for age > 60\n",
    "            .when((F.col(\"age_group\") == \"Under 30\") & (F.col(\"trestbps\") <= 130) & (F.col(\"chol\") <= 240), \"Low Risk\")  # Low risk condition for age < 30\n",
    "            .when((F.col(\"age_group\") == \"30-40\") & (F.col(\"trestbps\") <= 140) & (F.col(\"thalach\") >= 120), \"Low Risk\")  # Low risk condition for age between 30-40\n",
    "            .when((F.col(\"age_group\") == \"40-50\") & (F.col(\"chol\") <= 250) & (F.col(\"thalach\") >= 130), \"Low Risk\")  # Low risk condition for age between 40-50\n",
    "            .when((F.col(\"age_group\") == \"50-60\") & (F.col(\"trestbps\") <= 150) & (F.col(\"chol\") <= 260), \"Low Risk\")  # Low risk condition for age between 50-60\n",
    "            .when((F.col(\"age_group\") == \"60+\") & (F.col(\"trestbps\") <= 160) & (F.col(\"thalach\") >= 100), \"Low Risk\")  # Low risk condition for age > 60\n",
    "            .otherwise(\"Moderate Risk\")  # If none of the above conditions are met, assign \"Moderate Risk\"\n",
    "        )\n",
    "\n",
    "        # Group data by age group and risk profile, then count the number of occurrences in each group\n",
    "        risk_profile_counts = df.groupBy(\"age_group\", \"risk_profile\").count()\n",
    "\n",
    "        # Print the risk profile counts for each category\n",
    "        print(\"Risk profile counts by category:\")\n",
    "        risk_profile_counts.show()\n",
    "\n",
    "        # Save the result in Parquet format\n",
    "        risk_profile_counts.write.parquet(\"risk_profile_counts.parquet\")\n",
    "        print(\"Risk profile counts successfully saved in Parquet format.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Error handling in case of any issues during the risk profile creation or counting\n",
    "        print(f\"Error occurred while creating and counting risk profiles: {e}\")\n",
    "\n",
    "# Call the function to create and count risk profiles\n",
    "create_and_count_risk_profiles(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9f0397a-d04d-48f2-a009-9dc88de23b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relationship between chest pain type and presence of heart disease:\n",
      "+---+------+-----+----------+\n",
      "|cp |target|count|percentage|\n",
      "+---+------+-----+----------+\n",
      "|0  |0     |375  |75.45     |\n",
      "|0  |1     |122  |24.55     |\n",
      "|1  |0     |33   |19.76     |\n",
      "|1  |1     |134  |80.24     |\n",
      "|2  |0     |65   |22.89     |\n",
      "|2  |1     |219  |77.11     |\n",
      "|3  |0     |26   |33.77     |\n",
      "|3  |1     |51   |66.23     |\n",
      "+---+------+-----+----------+\n",
      "\n",
      "Results successfully saved in Parquet format.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window  # Import Window for windowed operations\n",
    "\n",
    "# Function to analyze the relationship between chest pain type and heart disease\n",
    "def analyze_chest_pain_and_heart_disease(df):\n",
    "    try:\n",
    "        # Group the data by chest pain type ('cp') and presence of heart disease ('target')\n",
    "        result = (\n",
    "            df.groupBy(\"cp\", \"target\")\n",
    "            .agg(F.count(\"*\").alias(\"count\"))  # Count the number of occurrences in each group\n",
    "            .withColumn(\n",
    "                \"percentage\",  # Calculate the percentage of each target within each chest pain type\n",
    "                F.round(\n",
    "                    F.col(\"count\") / F.sum(\"count\").over(Window.partitionBy(\"cp\")) * 100, \n",
    "                    2\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        \n",
    "        # The 'Window.partitionBy(\"cp\")' creates a window partitioned by chest pain type ('cp'),\n",
    "        # meaning that for each unique chest pain type, we are calculating the sum of counts for each group \n",
    "        # to get the percentage. This ensures that the percentage calculation is done within each chest pain type.\n",
    "        # The 'over(Window.partitionBy(\"cp\"))' ensures that the sum of counts is calculated for each partition (chest pain type)\n",
    "        # rather than for the entire dataset.\n",
    "\n",
    "        # Sort the result for better readability\n",
    "        result = result.orderBy(\"cp\", \"target\")\n",
    "\n",
    "        # Print the result\n",
    "        print(\"Relationship between chest pain type and presence of heart disease:\")\n",
    "        result.show(truncate=False)\n",
    "\n",
    "        # Save the result in Parquet format\n",
    "        result.write.parquet(\"chest_pain_heart_disease_analysis.parquet\")\n",
    "        print(\"Results successfully saved in Parquet format.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Error handling in case of any issues during the analysis\n",
    "        print(f\"Error occurred while analyzing the relationship between chest pain and heart disease: {e}\")\n",
    "\n",
    "# Call the function to analyze the relationship\n",
    "analyze_chest_pain_and_heart_disease(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565ec55a-3dd3-4282-a3f6-8f5633888a13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb68fdf-6a03-414c-bbf0-17b4220f2aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
